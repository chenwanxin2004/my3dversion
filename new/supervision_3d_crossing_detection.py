import numpy as np
import cv2 as cv
import time
import argparse
from collections import defaultdict, deque
import json
from datetime import datetime

try:
    import supervision as sv
    from ultralytics import YOLO
    SUPERVISION_AVAILABLE = True
except ImportError:
    SUPERVISION_AVAILABLE = False
    print("âš ï¸ Supervision or YOLO not available. Install with: pip install supervision ultralytics")

try:
    import pyrealsense2 as rs
    REALSENSE_AVAILABLE = True
except ImportError:
    REALSENSE_AVAILABLE = False
    print("âš ï¸ RealSense not available. Install with: pip install pyrealsense2")

try:
    from mediapipe_hand_pose import MediaPipeHandPose
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    print("âš ï¸ MediaPipe not available. Install with: pip install mediapipe")

try:
    from yolo_pose_detection import YOLOPoseDetector
    YOLO_POSE_AVAILABLE = True
except ImportError:
    YOLO_POSE_AVAILABLE = False
    print("âš ï¸ YOLO-pose not available. Check yolo_pose_detection.py")

class Supervision3DCrossingDetector:
    def __init__(self, model_path="yolov8n.pt"):
        """
        ç»“åˆSupervisionã€YOLOå’Œ3Dç‚¹äº‘çš„è¶Šçº¿æ£€æµ‹ç³»ç»Ÿ
        """
        self.pipeline = None
        self.config = None
        self.camera_intrinsics = None
        self.depth_scale = None
        
        # YOLO and Supervision setup
        self.model = None
        self.tracker = None
        self.line_zone = None
        self.line_zone_annotator = None
        self.box_annotator = None
        self.label_annotator = None
        
        # 3D point cloud parameters
        self.min_distance = 0.3
        self.max_distance = 5.0
        
        # ç®€åŒ–çš„3Dè¶Šçº¿æ£€æµ‹å‚æ•°
        self.crossing_distance = 1.0  # è¶Šçº¿è·ç¦»é˜ˆå€¼ï¼š1ç±³å†…ç®—è¶Šçº¿
        self.min_crossing_height = 0.3  # æœ€å°è¶Šçº¿é«˜åº¦ (meters)
        self.max_crossing_height = 2.0  # æœ€å¤§è¶Šçº¿é«˜åº¦ (meters)
        
        # æ”¾å®½æ£€æµ‹åŒºåŸŸé™åˆ¶ï¼Œé¿å…è¯¯åˆ¤
        self.detection_zone = {
            'min_x': -2.0,  # å·¦è¾¹ç•Œ (meters) - æ”¾å®½
            'max_x': 2.0,   # å³è¾¹ç•Œ (meters) - æ”¾å®½
            'min_z': 0.5,   # æœ€è¿‘è·ç¦» (meters) - æ”¾å®½
            'max_z': 5.0    # æœ€è¿œè·ç¦» (meters) - æ”¾å®½
        }
        
        # 2D visualization line (for display only)
        self.line_start = sv.Point(200, 300)  # 2Dæ˜¾ç¤ºçº¿èµ·ç‚¹
        self.line_end = sv.Point(600, 300)    # 2Dæ˜¾ç¤ºçº¿ç»ˆç‚¹
        self.line_thickness = 3
        
        # Statistics
        self.crossing_events = []
        self.person_count = 0
        self.unique_persons = set()
        self.person_tracks = defaultdict(lambda: {'first_seen': None, 'crossed': False, 'positions': deque(maxlen=10)})
        
        # Hand pose detection
        self.hand_pose_detector = None
        self.yolo_pose_detector = None  # YOLOv8-pose detector
        self.hand_gestures = []
        self.hand_gesture_history = deque(maxlen=10)
        
        # Initialize components
        self.init_realsense()
        self.init_yolo_supervision(model_path)
        self.init_mediapipe_hand_pose()
        self.init_yolo_pose_detector()
        
    def init_realsense(self):
        """Initialize Intel RealSense camera"""
        if not REALSENSE_AVAILABLE:
            print("âŒ RealSense not available")
            return
            
        try:
            # Configure depth and color streams
            self.pipeline = rs.pipeline()
            self.config = rs.config()
            
            # Enable depth and color streams
            self.config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
            self.config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
            
            # Start streaming
            profile = self.pipeline.start(self.config)
            
            # Get camera intrinsics
            depth_profile = rs.video_stream_profile(profile.get_stream(rs.stream.depth))
            depth_intrinsics = depth_profile.get_intrinsics()
            
            self.camera_intrinsics = np.array([
                [depth_intrinsics.fx, 0, depth_intrinsics.ppx],
                [0, depth_intrinsics.fy, depth_intrinsics.ppy],
                [0, 0, 1]
            ], dtype=np.float32)
            
            # Get depth scale
            depth_sensor = profile.get_device().first_depth_sensor()
            self.depth_scale = depth_sensor.get_depth_scale()
            
            print(f"âœ… RealSense camera initialized successfully")
            
        except Exception as e:
            print(f"âŒ Failed to initialize RealSense: {e}")
            self.pipeline = None
    
    def init_yolo_supervision(self, model_path):
        """Initialize YOLO model and Supervision components"""
        if not SUPERVISION_AVAILABLE:
            print("âŒ Supervision/YOLO not available")
            return
            
        try:
            # Load YOLO model
            self.model = YOLO(model_path)
            print(f"âœ… YOLO model loaded: {model_path}")
            
            # Initialize Supervision components
            self.tracker = sv.ByteTrack()
            # æ³¨æ„ï¼šæˆ‘ä»¬ä¸å†ä½¿ç”¨LineZoneè¿›è¡Œæ£€æµ‹ï¼Œåªç”¨äºå¯è§†åŒ–
            self.line_zone_annotator = sv.LineZoneAnnotator(thickness=self.line_thickness, text_thickness=1, text_scale=0.5)
            self.box_annotator = sv.BoxAnnotator(thickness=2)
            self.label_annotator = sv.LabelAnnotator(text_thickness=1, text_scale=0.5)
            
            print("âœ… Supervision components initialized")
            
        except Exception as e:
            print(f"âŒ Failed to initialize YOLO/Supervision: {e}")
            import traceback
            traceback.print_exc()
            self.model = None
    
    def init_mediapipe_hand_pose(self):
        """Initialize MediaPipe hand pose detection"""
        if not MEDIAPIPE_AVAILABLE:
            print("âŒ MediaPipe not available")
            return
            
        try:
            self.hand_pose_detector = MediaPipeHandPose(
                static_image_mode=False,
                max_num_hands=2,
                min_detection_confidence=0.7,
                min_tracking_confidence=0.5
            )
            print("âœ… MediaPipe hand pose detection initialized")
            
        except Exception as e:
            print(f"âŒ Failed to initialize MediaPipe hand pose: {e}")
            import traceback
            traceback.print_exc()
            self.hand_pose_detector = None
    
    def init_yolo_pose_detector(self):
        """Initialize YOLOv8-pose detector using existing module"""
        if not YOLO_POSE_AVAILABLE:
            print("âŒ YOLO-pose not available")
            return
            
        try:
            # ä½¿ç”¨ç°æœ‰çš„YOLOPoseDetectoræ¨¡å—
            self.yolo_pose_detector = YOLOPoseDetector("yolov8n-pose.pt")
            print("âœ… YOLOv8-pose detector initialized")
            
        except Exception as e:
            print(f"âŒ Failed to initialize YOLOv8-pose detector: {e}")
            import traceback
            traceback.print_exc()
            self.yolo_pose_detector = None
    
    def get_frames(self):
        """Get depth and color frames from RealSense"""
        if self.pipeline is None:
            return None, None
        
        try:
            # Wait for frames
            frames = self.pipeline.wait_for_frames()
            depth_frame = frames.get_depth_frame()
            color_frame = frames.get_color_frame()
            
            if not depth_frame or not color_frame:
                return None, None
            
            # Convert to numpy arrays
            depth_image = np.asanyarray(depth_frame.get_data())
            color_image = np.asanyarray(color_frame.get_data())
            
            # Convert depth to meters
            depth_image = depth_image.astype(np.float32) * self.depth_scale
            
            return depth_image, color_image
            
        except Exception as e:
            print(f"Error getting RealSense frames: {e}")
            return None, None
    
    def detect_persons_2d(self, color_image):
        """ä½¿ç”¨YOLOæ£€æµ‹2Då›¾åƒä¸­çš„äººç‰©"""
        if self.model is None:
            return [], []
        
        try:
            # Run YOLO inference
            results = self.model(color_image, verbose=False)
            
            # Get the first result (single image)
            if len(results) > 0:
                result = results[0]
            else:
                return [], []
            
            # Filter for person class (class_id = 0)
            detections = sv.Detections.from_ultralytics(result)
            
            # Check if we have any detections
            if len(detections) == 0:
                return detections, result
            
            # Filter for person class
            if hasattr(detections, 'class_id') and detections.class_id is not None:
                person_mask = detections.class_id == 0  # Person class
                detections = detections[person_mask]
            
            # Track persons
            detections = self.tracker.update_with_detections(detections)
            
            return detections, result
            
        except Exception as e:
            print(f"Error in YOLO detection: {e}")
            import traceback
            traceback.print_exc()
            return [], []
    
    def map_2d_to_3d(self, detections, depth_image):
        """å°†2Dæ£€æµ‹ç»“æœæ˜ å°„åˆ°3Dåæ ‡ï¼Œæ£€æµ‹æ•´ä¸ªè¾¹ç•Œæ¡†åŒºåŸŸ"""
        if self.camera_intrinsics is None or len(detections) == 0:
            return []
        
        fx, fy = self.camera_intrinsics[0, 0], self.camera_intrinsics[1, 1]
        cx, cy = self.camera_intrinsics[0, 2], self.camera_intrinsics[1, 2]
        
        person_3d_info = []
        
        # Check if we have valid detections
        if not hasattr(detections, 'xyxy') or not hasattr(detections, 'tracker_id'):
            return []
        
        for i, (bbox, track_id) in enumerate(zip(detections.xyxy, detections.tracker_id)):
            x1, y1, x2, y2 = bbox.astype(int)
            
            # ç¡®ä¿è¾¹ç•Œæ¡†åœ¨å›¾åƒèŒƒå›´å†…
            x1 = max(0, min(x1, depth_image.shape[1] - 1))
            y1 = max(0, min(y1, depth_image.shape[0] - 1))
            x2 = max(0, min(x2, depth_image.shape[1] - 1))
            y2 = max(0, min(y2, depth_image.shape[0] - 1))
            
            # æ”¹è¿›çš„æ·±åº¦æ£€æµ‹ï¼šæ›´æ™ºèƒ½çš„é‡‡æ ·å’Œè¿‡æ»¤
            center_x = (x1 + x2) // 2
            center_y = (y1 + y2) // 2
            
            # é¦–å…ˆè·å–ä¸­å¿ƒç‚¹æ·±åº¦ä½œä¸ºå‚è€ƒ
            center_depth = 0
            if (0 <= center_x < depth_image.shape[1] and 
                0 <= center_y < depth_image.shape[0]):
                center_depth = depth_image[center_y, center_x]
            
            # å¦‚æœä¸­å¿ƒç‚¹æ·±åº¦æ— æ•ˆï¼Œè·³è¿‡è¿™ä¸ªæ£€æµ‹
            if center_depth <= 0.1 or center_depth > 10.0:
                continue
            
            # åŸºäºä¸­å¿ƒç‚¹æ·±åº¦ï¼Œåªé‡‡æ ·åˆç†èŒƒå›´å†…çš„ç‚¹
            depth_tolerance = 1.0  # å…è®¸1ç±³çš„æ·±åº¦å·®å¼‚
            min_valid_depth = max(0.1, center_depth - depth_tolerance)
            max_valid_depth = min(10.0, center_depth + depth_tolerance)
            
            valid_depths = [center_depth]
            min_depth = center_depth
            max_depth = center_depth
            
            # é‡‡æ ·ç­–ç•¥ï¼šåªé‡‡æ ·ä¸­å¿ƒåŒºåŸŸï¼Œé¿å…è¾¹ç•Œå™ªå£°
            sample_points = [
                (center_x, center_y),  # ä¸­å¿ƒç‚¹
                (center_x - 20, center_y),  # å·¦
                (center_x + 20, center_y),  # å³
                (center_x, center_y - 20),  # ä¸Š
                (center_x, center_y + 20),  # ä¸‹
            ]
            
            for px, py in sample_points:
                if (0 <= px < depth_image.shape[1] and 
                    0 <= py < depth_image.shape[0]):
                    
                    depth = depth_image[py, px]
                    
                    # æ›´ä¸¥æ ¼çš„æ·±åº¦è¿‡æ»¤ï¼šå¿…é¡»åœ¨åˆç†èŒƒå›´å†…
                    if min_valid_depth <= depth <= max_valid_depth:
                        valid_depths.append(depth)
                        min_depth = min(min_depth, depth)
                        max_depth = max(max_depth, depth)
            
            # å¦‚æœæœ‰æœ‰æ•ˆæ·±åº¦å€¼
            if valid_depths and center_depth > 0:
                # ä½¿ç”¨æœ€å°æ·±åº¦ä½œä¸ºæ£€æµ‹æ ‡å‡†ï¼ˆæœ€æ¥è¿‘ç›¸æœºçš„éƒ¨åˆ†ï¼‰
                detection_depth = min_depth
                
                if self.min_distance < detection_depth < self.max_distance:
                    # è®¡ç®—ä¸­å¿ƒç‚¹çš„3Dåæ ‡
                    center_x = (x1 + x2) // 2
                    center_y = (y1 + y2) // 2
                    
                    x_3d = (center_x - cx) * center_depth / fx
                    y_3d = (center_y - cy) * center_depth / fy
                    z_3d = center_depth
                    
                    person_3d_info.append({
                        'track_id': track_id,
                        'bbox_2d': bbox,
                        'center_2d': (center_x, center_y),
                        'position_3d': (x_3d, y_3d, z_3d),
                        'depth': center_depth,  # ä¸­å¿ƒç‚¹æ·±åº¦
                        'min_depth': min_depth,  # æœ€å°æ·±åº¦ï¼ˆæœ€æ¥è¿‘ç›¸æœºçš„éƒ¨åˆ†ï¼‰
                        'max_depth': max_depth,  # æœ€å¤§æ·±åº¦
                        'confidence': detections.confidence[i] if hasattr(detections, 'confidence') else 1.0
                    })
        
        return person_3d_info
    
    def detect_hand_gestures(self, color_image):
        """æ£€æµ‹æ‰‹éƒ¨å§¿æ€å’Œæ‰‹åŠ¿"""
        if self.hand_pose_detector is None:
            return []
        
        try:
            # æ£€æµ‹æ‰‹éƒ¨
            hands_info = self.hand_pose_detector.detect_hands(color_image)
            
            # æ›´æ–°æ‰‹åŠ¿å†å²
            current_gestures = []
            for hand in hands_info:
                gesture_info = {
                    'hand_id': hand['hand_id'],
                    'label': hand['label'],
                    'gesture': hand['gesture'],
                    'confidence': hand['confidence'],
                    'center': hand['center'],
                    'bbox': hand['bbox'],
                    'pose_3d': hand['pose_3d']
                }
                current_gestures.append(gesture_info)
            
            self.hand_gesture_history.append(current_gestures)
            
            return hands_info
            
        except Exception as e:
            print(f"Error in hand gesture detection: {e}")
            return []
    
    def get_gesture_info(self, hands_info):
        """è·å–æ‰‹åŠ¿ä¿¡æ¯ï¼ˆä¸ç”Ÿæˆæ§åˆ¶å‘½ä»¤ï¼‰"""
        gesture_info = []
        
        for hand in hands_info:
            gesture = hand['gesture']
            hand_id = hand['hand_id']
            label = hand['label']
            confidence = hand['confidence']
            
            # åªè®°å½•æ‰‹åŠ¿ä¿¡æ¯ï¼Œä¸ç”Ÿæˆå‘½ä»¤
            gesture_info.append({
                'hand_id': hand_id,
                'label': label,
                'gesture': gesture,
                'confidence': confidence,
                'center': hand['center'],
                'bbox': hand['bbox']
            })
        
        return gesture_info
    
    def detect_poses(self, color_image):
        """ä½¿ç”¨YOLOv8-poseæ£€æµ‹äººä½“å§¿åŠ¿"""
        if self.yolo_pose_detector is None:
            return []
        
        try:
            # ä½¿ç”¨ç°æœ‰æ¨¡å—çš„æ£€æµ‹æ–¹æ³•
            poses_info = self.yolo_pose_detector.detect_poses(color_image)
            return poses_info
            
        except Exception as e:
            print(f"Error in YOLOv8-pose detection: {e}")
            return []
    
    
    def project_3d_to_2d(self, point_3d):
        """å°†3Dç‚¹æŠ•å½±åˆ°2Då›¾åƒåæ ‡"""
        if self.camera_intrinsics is None:
            return None
        
        x, y, z = point_3d
        
        # æŠ•å½±åˆ°2D
        u = int(self.camera_intrinsics[0, 0] * x / z + self.camera_intrinsics[0, 2])
        v = int(self.camera_intrinsics[1, 1] * y / z + self.camera_intrinsics[1, 2])
        
        return (u, v)
    
    def check_line_crossing_3d(self, person_3d_info):
        """ç®€åŒ–çš„3Dè¶Šçº¿æ£€æµ‹ï¼šè·ç¦»åœ¨1ç±³å†…å°±ç®—è¶Šçº¿"""
        crossing_events = []
        
        for person in person_3d_info:
            track_id = person['track_id']
            position_3d = person['position_3d']
            center_2d = person['center_2d']
            depth = person['depth']
            
            # Update person track
            if track_id not in self.person_tracks:
                self.person_tracks[track_id]['first_seen'] = datetime.now()
            
            self.person_tracks[track_id]['positions'].append(center_2d)
            
            # ç®€åŒ–çš„è¶Šçº¿æ£€æµ‹ï¼šäººä½“ä»»ä½•éƒ¨åˆ†åœ¨1ç±³å†…å°±ç®—è¶Šçº¿
            if not self.person_tracks[track_id]['crossed']:
                if self.is_person_crossing_simple(person):
                    self.person_tracks[track_id]['crossed'] = True
                    self.unique_persons.add(track_id)
                    
                    crossing_event = {
                        'track_id': track_id,
                        'timestamp': datetime.now(),
                        'position_3d': position_3d,
                        'position_2d': center_2d,
                        'depth': depth,
                        'direction': self.get_crossing_direction_simple(position_3d)
                    }
                    
                    crossing_events.append(crossing_event)
                    self.crossing_events.append(crossing_event)
        
        return crossing_events
    
    def is_person_crossing_simple(self, person_info):
        """æ”¹è¿›çš„è¶Šçº¿æ£€æµ‹ï¼šæ›´åˆç†çš„åˆ¤æ–­é€»è¾‘"""
        position_3d = person_info['position_3d']
        center_depth = person_info['depth']
        min_depth = person_info.get('min_depth', center_depth)
        max_depth = person_info.get('max_depth', center_depth)
        x, y, z = position_3d
        
        # 1. æ£€æŸ¥é«˜åº¦æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
        if not (self.min_crossing_height <= y <= self.max_crossing_height):
            return False
        
        # 2. æ£€æŸ¥æ˜¯å¦åœ¨æ£€æµ‹åŒºåŸŸå†…
        zone = self.detection_zone
        if not (zone['min_x'] <= x <= zone['max_x'] and 
                zone['min_z'] <= z <= zone['max_z']):
            return False
        
        # 3. æ”¹è¿›çš„è¶Šçº¿åˆ¤æ–­é€»è¾‘ï¼š
        # - å¦‚æœä¸­å¿ƒæ·±åº¦åœ¨1ç±³å†…ï¼Œç›´æ¥è¶Šçº¿
        # - å¦‚æœä¸­å¿ƒæ·±åº¦åœ¨1-2ç±³å†…ï¼Œä¸”æœ€å°æ·±åº¦åœ¨1ç±³å†…ï¼Œä¹Ÿç®—è¶Šçº¿
        # - å¦‚æœæ·±åº¦èŒƒå›´è¿‡å¤§ï¼ˆå¯èƒ½æ˜¯å™ªå£°ï¼‰ï¼Œä½¿ç”¨ä¸­å¿ƒæ·±åº¦åˆ¤æ–­
        
        depth_range = max_depth - min_depth
        
        # å¦‚æœæ·±åº¦èŒƒå›´è¿‡å¤§ï¼ˆ>2ç±³ï¼‰ï¼Œå¯èƒ½æ˜¯å™ªå£°ï¼Œä½¿ç”¨ä¸­å¿ƒæ·±åº¦
        if depth_range > 2.0:
            return center_depth <= self.crossing_distance
        
        # æ­£å¸¸æƒ…å†µï¼šä½¿ç”¨æœ€å°æ·±åº¦åˆ¤æ–­
        return min_depth <= self.crossing_distance
    
    def get_crossing_direction_simple(self, position_3d):
        """è·å–è¶Šçº¿æ–¹å‘ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        x, y, z = position_3d
        
        # åŸºäºxåæ ‡åˆ¤æ–­æ–¹å‘
        if x < -0.5:
            return "From Left"
        elif x > 0.5:
            return "From Right"
        else:
            return "From Center"
    
    def create_visualization(self, color_image, detections, person_3d_info, crossing_events, detection_summary):
        """åˆ›å»ºå¯è§†åŒ–ç•Œé¢"""
        # Annotate detections
        if len(detections) > 0:
            labels = [f"Person {track_id}" for track_id in detections.tracker_id]
            annotated_image = self.box_annotator.annotate(color_image.copy(), detections)
            annotated_image = self.label_annotator.annotate(annotated_image, detections, labels)
        else:
            annotated_image = color_image.copy()
        
        # æ·»åŠ æ‰‹éƒ¨æ£€æµ‹å¯è§†åŒ–ï¼ˆMediaPipeï¼‰
        if detection_summary['hands']['data'] and self.hand_pose_detector:
            annotated_image = self.hand_pose_detector.visualize_hands(annotated_image, detection_summary['hands']['data'])
        
        # æ·»åŠ YOLOv8-poseå¯è§†åŒ–
        if detection_summary['poses']['data'] and self.yolo_pose_detector:
            annotated_image = self.yolo_pose_detector.visualize_poses(annotated_image, detection_summary['poses']['data'])
        
        # ç§»é™¤æ‰€æœ‰å¤šä½™çš„çº¿æ¡ï¼ŒçœŸæ­£çš„æ£€æµ‹åŸºäºæ·±åº¦å€¼
        
        # Add 3D information overlay with depth highlighting
        for person in person_3d_info:
            center_2d = person['center_2d']
            position_3d = person['position_3d']
            track_id = person['track_id']
            depth = person['depth']
            min_depth = person.get('min_depth', depth)
            
            # æ”¹è¿›çš„è¶Šçº¿åˆ¤æ–­é€»è¾‘
            depth_range = person.get('max_depth', depth) - min_depth
            is_crossing = False
            
            # ä½¿ç”¨ä¸æ£€æµ‹é€»è¾‘ç›¸åŒçš„åˆ¤æ–­
            if depth_range > 2.0:  # æ·±åº¦èŒƒå›´è¿‡å¤§ï¼Œä½¿ç”¨ä¸­å¿ƒæ·±åº¦
                is_crossing = depth <= self.crossing_distance
                detection_method = "Center"
            else:  # æ­£å¸¸æƒ…å†µï¼Œä½¿ç”¨æœ€å°æ·±åº¦
                is_crossing = min_depth <= self.crossing_distance
                detection_method = "Min"
            
            # è®¾ç½®é¢œè‰²å’ŒçŠ¶æ€
            if is_crossing:
                color = (0, 0, 255)  # çº¢è‰²ï¼šè¶Šçº¿
                status = "CROSSING!"
            else:
                color = (0, 255, 0)  # ç»¿è‰²ï¼šæ­£å¸¸
                status = "Normal"
            
            # æ·»åŠ æ·±åº¦ä¿¡æ¯æ–‡æœ¬
            depth_text = f"ID:{track_id} {detection_method}:{min_depth if detection_method == 'Min' else depth:.2f}m {status}"
            cv.putText(annotated_image, depth_text, 
                      (int(center_2d[0] - 80), int(center_2d[1] - 10)), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            
            # æ·»åŠ è¯¦ç»†æ·±åº¦ä¿¡æ¯
            detail_text = f"Center:{depth:.2f}m Range:{min_depth:.2f}-{person.get('max_depth', depth):.2f}m"
            cv.putText(annotated_image, detail_text, 
                      (int(center_2d[0] - 80), int(center_2d[1] + 15)), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        
        # æ·»åŠ æ‰‹åŠ¿ä¿¡æ¯å¯è§†åŒ–ï¼ˆä»æ‘˜è¦ä¸­è·å–ï¼‰
        if detection_summary['hands']['gestures']:
            for i, gesture in enumerate(detection_summary['hands']['gestures']):
                gesture_cn = self._translate_gesture_to_chinese(gesture['gesture'])
                hand_cn = "å·¦æ‰‹" if gesture['label'] == 'Left' else "å³æ‰‹"
                gesture_text = f"æ‰‹åŠ¿: {gesture_cn} ({hand_cn})"
                cv.putText(annotated_image, gesture_text, (10, 200 + i * 25), 
                          cv.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        
        # ç§»é™¤ç»Ÿè®¡ä¿¡æ¯è¦†ç›–å±‚ï¼Œæ”¹ä¸ºåœ¨ç‹¬ç«‹çª—å£ä¸­æ˜¾ç¤º
        
        return annotated_image
    
    def create_statistics_panel(self, detection_summary):
        """åˆ›å»ºç¾è§‚çš„ç»Ÿè®¡ä¿¡æ¯é¢æ¿"""
        panel_height = 500
        panel_width = 600
        panel = np.zeros((panel_height, panel_width, 3), dtype=np.uint8)
        
        # è®¾ç½®æ¸å˜èƒŒæ™¯
        for y in range(panel_height):
            intensity = int(30 + (y / panel_height) * 20)
            panel[y, :] = (intensity, intensity, intensity)
        
        # ç»˜åˆ¶æ ‡é¢˜åŒºåŸŸ
        cv.rectangle(panel, (10, 10), (590, 60), (0, 100, 200), -1)
        cv.putText(panel, "3Dè¶Šçº¿æ£€æµ‹ç³»ç»Ÿ", (50, 40), 
                  cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
        
        # ç»˜åˆ¶åˆ†éš”çº¿
        cv.line(panel, (20, 80), (580, 80), (100, 100, 100), 2)
        
        # å®æ—¶ç»Ÿè®¡ä¿¡æ¯
        y_pos = 110
        cv.putText(panel, "å®æ—¶ç»Ÿè®¡ä¿¡æ¯", (30, y_pos), 
                  cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        y_pos += 40
        
        # å½“å‰å¸§ä¿¡æ¯ï¼ˆä»æ‘˜è¦ä¸­è·å–ï¼‰
        current_stats = [
            ("å½“å‰å¸§äººæ•°", detection_summary['persons']['count'], (0, 255, 0)),
            ("æ£€æµ‹åˆ°æ‰‹éƒ¨", detection_summary['hands']['count'], (255, 255, 0)),
            ("è¯†åˆ«å‡ºæ‰‹åŠ¿", len(detection_summary['hands']['gestures']), (255, 165, 0)),
            ("æ£€æµ‹åˆ°å§¿åŠ¿", detection_summary['poses']['count'], (255, 0, 255))
        ]
        
        for label, value, color in current_stats:
            cv.putText(panel, f"{label}:", (50, y_pos), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2)
            cv.putText(panel, str(value), (350, y_pos), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            y_pos += 30
        
        # å†å²ç»Ÿè®¡ä¿¡æ¯
        y_pos += 20
        cv.putText(panel, "å†å²ç»Ÿè®¡ä¿¡æ¯", (30, y_pos), 
                  cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        y_pos += 40
        
        historical_stats = [
            ("ç´¯è®¡æ£€æµ‹äººæ•°", detection_summary['statistics']['total_persons'], (0, 255, 0)),
            ("è¶Šçº¿äº‹ä»¶æ€»æ•°", detection_summary['statistics']['crossing_events'], (0, 0, 255)),
            ("æ´»è·ƒè·Ÿè¸ªæ•°", detection_summary['statistics']['active_tracks'], (255, 255, 0))
        ]
        
        for label, value, color in historical_stats:
            cv.putText(panel, f"{label}:", (50, y_pos), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2)
            cv.putText(panel, str(value), (350, y_pos), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
            y_pos += 30
        
        # æœ€è¿‘çš„æ‰‹åŠ¿ä¿¡æ¯ï¼ˆä»æ‘˜è¦ä¸­è·å–ï¼‰
        if detection_summary['hands']['gestures']:
            y_pos += 20
            cv.putText(panel, "æœ€è¿‘æ‰‹åŠ¿", (30, y_pos), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
            y_pos += 30
            
            for i, gesture in enumerate(detection_summary['hands']['gestures'][:3]):  # åªæ˜¾ç¤ºæœ€è¿‘3ä¸ª
                # å°†æ‰‹åŠ¿åç§°ç¿»è¯‘ä¸ºä¸­æ–‡
                gesture_cn = self._translate_gesture_to_chinese(gesture['gesture'])
                hand_cn = "å·¦æ‰‹" if gesture['label'] == 'Left' else "å³æ‰‹"
                gesture_text = f"  {hand_cn}: {gesture_cn}"
                cv.putText(panel, gesture_text, (50, y_pos), 
                          cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
                y_pos += 25
        
        # æœ€è¿‘çš„è¶Šçº¿äº‹ä»¶
        if self.crossing_events:
            y_pos += 20
            cv.putText(panel, "æœ€è¿‘è¶Šçº¿äº‹ä»¶", (30, y_pos), 
                      cv.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
            y_pos += 30
            
            recent_events = self.crossing_events[-3:]  # åªæ˜¾ç¤ºæœ€è¿‘3ä¸ª
            for event in recent_events:
                direction = event.get('direction', 'Unknown')
                # å°†æ–¹å‘ç¿»è¯‘ä¸ºä¸­æ–‡
                direction_cn = self._translate_direction_to_chinese(direction)
                event_text = f"  äººå‘˜ {event['track_id']} ({direction_cn})"
                cv.putText(panel, event_text, (50, y_pos), 
                          cv.FONT_HERSHEY_SIMPLEX, 0.5, (255, 100, 100), 1)
                y_pos += 25
        
        # æ·»åŠ è¾¹æ¡†
        cv.rectangle(panel, (5, 5), (595, 495), (100, 100, 100), 2)
        
        return panel
    
    def _translate_gesture_to_chinese(self, gesture):
        """å°†æ‰‹åŠ¿åç§°ç¿»è¯‘ä¸ºä¸­æ–‡"""
        gesture_translations = {
            'Fist': 'æ¡æ‹³',
            'Open Hand': 'å¼ å¼€æ‰‹æŒ',
            'Pointing': 'æŒ‡å‘',
            'Peace Sign': 'èƒœåˆ©æ‰‹åŠ¿',
            'OK Sign': 'OKæ‰‹åŠ¿',
            'Thumbs Up': 'ç«–æ‹‡æŒ‡',
            'Other': 'å…¶ä»–æ‰‹åŠ¿',
            'Unknown': 'æœªçŸ¥æ‰‹åŠ¿'
        }
        return gesture_translations.get(gesture, gesture)
    
    def _translate_direction_to_chinese(self, direction):
        """å°†æ–¹å‘åç§°ç¿»è¯‘ä¸ºä¸­æ–‡"""
        direction_translations = {
            'From Left': 'ä»å·¦ä¾§',
            'From Right': 'ä»å³ä¾§',
            'From Center': 'ä»ä¸­å¤®',
            'Unknown': 'æœªçŸ¥æ–¹å‘'
        }
        return direction_translations.get(direction, direction)
    
    def create_detection_summary(self, person_3d_info, hands_info, poses_info):
        """åˆ›å»ºç»Ÿä¸€çš„æ£€æµ‹ä¿¡æ¯æ‘˜è¦ï¼Œå‡å°‘å‚æ•°å†—ä½™"""
        summary = {
            # äººå‘˜ä¿¡æ¯
            'persons': {
                'count': len(person_3d_info),
                'data': person_3d_info
            },
            
            # æ‰‹éƒ¨æ£€æµ‹ä¿¡æ¯ï¼ˆMediaPipeï¼‰
            'hands': {
                'count': len(hands_info) if hands_info else 0,
                'data': hands_info if hands_info else [],
                'gestures': []
            },
            
            # å§¿åŠ¿æ£€æµ‹ä¿¡æ¯ï¼ˆYOLOv8-poseï¼‰
            'poses': {
                'count': len(poses_info) if poses_info else 0,
                'data': poses_info if poses_info else [],
                'hand_actions': []
            },
            
            # ç»Ÿè®¡ä¿¡æ¯
            'statistics': {
                'total_persons': len(self.unique_persons),
                'crossing_events': len(self.crossing_events),
                'active_tracks': len(self.person_tracks)
            }
        }
        
        # æå–æ‰‹åŠ¿ä¿¡æ¯
        if hands_info:
            for hand in hands_info:
                summary['hands']['gestures'].append({
                    'label': hand['label'],
                    'gesture': hand['gesture'],
                    'confidence': hand['confidence']
                })
        
        # æå–æ‰‹éƒ¨åŠ¨ä½œä¿¡æ¯
        if poses_info:
            for pose in poses_info:
                summary['poses']['hand_actions'].append({
                    'person_id': pose['person_id'],
                    'left_hand': pose['hand_actions']['left_hand'],
                    'right_hand': pose['hand_actions']['right_hand']
                })
        
        return summary
    
    
    def colorize_depth(self, depth_image):
        """å°†æ·±åº¦å›¾è½¬æ¢ä¸ºå½©è‰²å›¾åƒ"""
        # é™åˆ¶æ·±åº¦èŒƒå›´åˆ°0-5ç±³
        depth_clipped = np.clip(depth_image, 0, 5.0)
        
        # å½’ä¸€åŒ–åˆ°0-255
        depth_normalized = (depth_clipped / 5.0 * 255).astype(np.uint8)
        
        # åº”ç”¨é¢œè‰²æ˜ å°„ï¼ˆè¿‘å¤„çº¢è‰²ï¼Œè¿œå¤„è“è‰²ï¼‰
        depth_colored = cv.applyColorMap(depth_normalized, cv.COLORMAP_JET)
        
        # æ·»åŠ è·ç¦»æ ‡æ³¨
        height, width = depth_colored.shape[:2]
        cv.putText(depth_colored, "Depth Map (Red=Close, Blue=Far)", 
                  (10, 30), cv.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
        
        # æ·»åŠ è·ç¦»åˆ»åº¦
        for i, distance in enumerate([1.0, 2.0, 3.0, 4.0, 5.0]):
            y_pos = int(height * (1 - distance / 5.0))
            cv.line(depth_colored, (width - 100, y_pos), (width - 80, y_pos), (255, 255, 255), 1)
            cv.putText(depth_colored, f"{distance}m", 
                      (width - 75, y_pos + 5), cv.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)
        
        return depth_colored
    
    def save_statistics(self, filename="crossing_statistics.json"):
        """ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶"""
        stats = {
            'total_unique_persons': len(self.unique_persons),
            'total_crossing_events': len(self.crossing_events),
            'crossing_events': [
                {
                    'track_id': event['track_id'],
                    'timestamp': event['timestamp'].isoformat(),
                    'position_3d': event['position_3d'],
                    'position_2d': event['position_2d'],
                    'depth': event['depth']
                }
                for event in self.crossing_events
            ],
            'generated_at': datetime.now().isoformat()
        }
        
        with open(filename, 'w') as f:
            json.dump(stats, f, indent=2)
        
        print(f"âœ… Statistics saved to {filename}")
    
    def cleanup(self):
        """Clean up resources"""
        if self.pipeline:
            self.pipeline.stop()
        if self.hand_pose_detector:
            self.hand_pose_detector.cleanup()
        if self.yolo_pose_detector:
            self.yolo_pose_detector.cleanup()
        cv.destroyAllWindows()

def main():
    parser = argparse.ArgumentParser(description='Supervision + YOLO + 3D Point Cloud Crossing Detection')
    parser.add_argument('--model', default='yolov8n.pt', help='YOLO model path')
    parser.add_argument('--save-stats', action='store_true', help='Save statistics to file')
    
    args = parser.parse_args()
    
    print("3D Crossing Detection + Hand Gesture Recognition")
    print("=" * 60)
    print("Features:")
    print("- YOLO person detection")
    print("- RealSense depth information display")
    print("- Simple rule: Distance <= 1m = Crossing")
    print("- Height filtering (0.3-2.0m)")
    print("- Detection zone: 1-3m range, Â±1m width")
    print("- Real-time depth map visualization")
    print("- Person counting and tracking")
    print("- MediaPipe hand gesture recognition")
    print("- YOLOv8-pose body pose detection")
    print("- Real-time gesture and pose display")
    print("Press 'q' or ESC to quit, 's' to save statistics")
    
    # Initialize detector
    detector = Supervision3DCrossingDetector(args.model)
    
    if detector.pipeline is None:
        print("âŒ Failed to initialize RealSense camera")
        return
    
    if detector.model is None:
        print("âŒ Failed to initialize YOLO/Supervision")
        return
    
    try:
        frame_count = 0
        while True:
            frame_count += 1
            
            # Get frames from RealSense
            depth_image, color_image = detector.get_frames()
            
            if depth_image is None or color_image is None:
                print("âŒ Failed to get frames from RealSense")
                time.sleep(0.1)
                continue
            
            # Detect persons in 2D
            detections, yolo_results = detector.detect_persons_2d(color_image)
            
            # Map 2D detections to 3D coordinates
            person_3d_info = detector.map_2d_to_3d(detections, depth_image)
            
            # Check for 3D line crossing
            crossing_events = detector.check_line_crossing_3d(person_3d_info)
            
            # Detect hand gestures (MediaPipe) - å¯é€‰ï¼Œä¸YOLOv8-poseäºŒé€‰ä¸€
            hands_info = detector.detect_hand_gestures(color_image)
            
            # Detect poses (YOLOv8-pose) - åŒ…å«æ‰‹éƒ¨åŠ¨ä½œæ£€æµ‹
            poses_info = detector.detect_poses(color_image)
            
            # åˆå¹¶æ£€æµ‹ä¿¡æ¯ï¼Œå‡å°‘å†—ä½™
            detection_summary = detector.create_detection_summary(person_3d_info, hands_info, poses_info)
            
            # Create visualizations
            annotated_image = detector.create_visualization(color_image, detections, person_3d_info, crossing_events, detection_summary)
            stats_panel = detector.create_statistics_panel(detection_summary)
            
            # Display results in multiple windows
            cv.imshow('Crossing Detection', annotated_image)
            cv.imshow('Statistics', stats_panel)
            
            # æ˜¾ç¤ºæ·±åº¦å›¾
            depth_colored = detector.colorize_depth(depth_image)
            cv.imshow('Depth Map', depth_colored)
            
            # Print crossing events
            if crossing_events:
                for event in crossing_events:
                    direction = event.get('direction', 'Unknown')
                    direction_cn = detector._translate_direction_to_chinese(direction)
                    print(f"ğŸš¨ äººå‘˜ {event['track_id']} åœ¨ {event['timestamp'].strftime('%H:%M:%S')} è¶Šçº¿ ({direction_cn})")
            
            # Print gesture information
            if detection_summary['hands']['gestures']:
                for gesture in detection_summary['hands']['gestures']:
                    gesture_cn = detector._translate_gesture_to_chinese(gesture['gesture'])
                    hand_cn = "å·¦æ‰‹" if gesture['label'] == 'Left' else "å³æ‰‹"
                    print(f"ğŸ‘‹ æ£€æµ‹åˆ°æ‰‹åŠ¿: {gesture_cn} ({hand_cn}) (ç½®ä¿¡åº¦: {gesture['confidence']:.2f})")
            
            # Print pose information
            if detection_summary['poses']['hand_actions']:
                for hand_action in detection_summary['poses']['hand_actions']:
                    left_action = hand_action['left_hand']['action']
                    right_action = hand_action['right_hand']['action']
                    print(f"ğŸ¤¸ æ£€æµ‹åˆ°å§¿åŠ¿: äººå‘˜ {hand_action['person_id']} - å·¦æ‰‹: {left_action}, å³æ‰‹: {right_action}")
            
            # Handle key presses
            key = cv.waitKey(30) & 0xFF
            if key == ord('q') or key == 27:  # 'q' or ESC
                break
            elif key == ord('s'):  # Save statistics
                detector.save_statistics()
    
    except KeyboardInterrupt:
        print("\nStopping...")
    
    finally:
        if args.save_stats:
            detector.save_statistics()
        detector.cleanup()
        print("Crossing detection stopped.")

if __name__ == "__main__":
    main()